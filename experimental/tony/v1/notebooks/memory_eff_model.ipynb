{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "DATA_PATH = \"/projects/jlab/to.shen/cgflow-dev/experiments/data/complex/plinder_15A\"\n",
                "\n",
                "# Dataset name\n",
                "DATASET = \"plinder\"\n",
                "\n",
                "# Number of molecules to evaluate\n",
                "NUM_EVAL_MOLS = np.inf\n",
                "\n",
                "# Number of inference steps\n",
                "NUM_INFERENCE_STEPS = 100\n",
                "\n",
                "# Whether the data involves protein-ligand complexes\n",
                "IS_COMPLEX = DATASET in [\"plinder\", \"crossdock\", \"zinc15m\"] or False\n",
                "\n",
                "# Create a class to simulate command line arguments\n",
                "class Args:\n",
                "    def __init__(self):\n",
                "        pass\n",
                "\n",
                "args = Args()\n",
                "\n",
                "# Set required arguments\n",
                "args.data_path = DATA_PATH\n",
                "args.dataset = DATASET\n",
                "args.n_validation_mols = NUM_EVAL_MOLS\n",
                "args.num_inference_steps = NUM_INFERENCE_STEPS\n",
                "args.num_gpus = 1\n",
                "args.is_pseudo_complex = False\n",
                "args.batch_cost = 8\n",
                "args.use_complex_metrics = IS_COMPLEX\n",
                "args.sampling_strategy = \"linear\"\n",
                "args.num_workers = 0\n",
                "\n",
                "# Model architecture parameters - these should match the trained model\n",
                "args.d_model = 384\n",
                "args.n_layers = 12\n",
                "args.d_message = 64\n",
                "args.d_edge = 128\n",
                "args.n_coord_sets = 64\n",
                "args.n_attn_heads = 32\n",
                "args.d_message_hidden = 96\n",
                "args.coord_norm = \"length\"\n",
                "args.size_emb = 64\n",
                "args.max_atoms = 256\n",
                "args.pocket_n_layers = 4\n",
                "args.pocket_d_inv = 256\n",
                "args.fixed_equi = False\n",
                "\n",
                "# Flow matching parameters\n",
                "args.categorical_strategy = \"auto-regressive\"\n",
                "args.conf_coord_strategy = \"gaussian\"\n",
                "args.optimal_transport = None\n",
                "args.cat_sampling_noise_level = 1\n",
                "args.coord_noise_std_dev = 0.2\n",
                "args.type_dist_temp = 1.0\n",
                "args.time_alpha = 1.0\n",
                "args.time_beta = 1.0\n",
                "args.dist_loss_weight = 0.0\n",
                "args.type_loss_weight = 0.0\n",
                "args.bond_loss_weight = 0.0\n",
                "args.charge_loss_weight = 0.0\n",
                "args.monitor = \"val-strain\"\n",
                "args.monitor_mode = \"min\"\n",
                "args.val_check_epochs = 1\n",
                "\n",
                "\n",
                "# Autoregressive parameters (only needed if model was trained with AR)\n",
                "args.t_per_ar_action = 0.33  # updated\n",
                "args.max_interp_time = 1.0  # updated\n",
                "args.decomposition_strategy = \"reaction\"  # updated\n",
                "args.ordering_strategy = \"connected\"  # updated\n",
                "args.max_action_t = 0.66  # updated\n",
                "args.max_num_cuts = 2  # updated\n",
                "args.min_group_size = 5\n",
                "\n",
                "# Model loading defaults\n",
                "args.arch = \"semla\"\n",
                "args.trial_run = False\n",
                "args.use_ema = True\n",
                "args.self_condition = True\n",
                "args.lr = 0.0003\n",
                "args.type_loss_weight = 0.0  # updated\n",
                "args.bond_loss_weight = 0.0  # updated\n",
                "args.charge_loss_weight = 0.0  # updated\n",
                "args.dist_loss_weight = 0.0  # updated\n",
                "args.lr_schedule = \"constant\"\n",
                "args.warm_up_steps = 10000\n",
                "args.bucket_cost_scale = \"linear\"\n",
                "args.epochs = 1\n",
                "args.acc_batches = 1\n",
                "args.val_check_epochs = 1  # updated\n",
                "args.gradient_clip_val = 1.0\n",
                "args.monitor = \"val-strain\"  # updated\n",
                "args.monitor_mode = \"min\"  # updated\n",
                "\n",
                "args.n_training_mols = np.inf\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "# Fixed or default parameters\n",
                "fixed_equi = False\n",
                "pocket_d_equi = 1 if fixed_equi else 64\n",
                "pocket_d_inv = 256\n",
                "pocket_n_layers = 4\n",
                "\n",
                "# Model hyperparameters\n",
                "d_model = 384\n",
                "n_layers = 12\n",
                "d_message = 128\n",
                "d_edge = 128\n",
                "n_coord_sets = 64\n",
                "n_attn_heads = 32\n",
                "d_message_hidden = 128\n",
                "self_condition = False\n",
                "# Vocabulary and bond types\n",
                "n_extra_atom_feats = 1\n",
                "n_res_types = 21\n",
                "\n",
                "PLINDER_STD_DEV = 2.2693647416252976\n",
                "PLINDER_BUCKET_LIMITS = [\n",
                "    96,\n",
                "    125,\n",
                "    149,\n",
                "    166,\n",
                "    179,\n",
                "    189,\n",
                "    199,\n",
                "    208,\n",
                "    216,\n",
                "    223,\n",
                "    231,\n",
                "    239,\n",
                "    248,\n",
                "    258,\n",
                "    269,\n",
                "    283,\n",
                "    300,\n",
                "    324,\n",
                "    377,\n",
                "    978\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using type ARGeometricComplexInterpolant for training\n"
                    ]
                }
            ],
            "source": [
                "import cgflow.scriptutil as util\n",
                "from cgflow.buildutil import build_dm\n",
                "vocab = util.build_vocab()\n",
                "dm = build_dm(\n",
                "    args,\n",
                "    vocab,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "pickle.dump(dm, open(\"dm.pkl\", \"wb\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dm = pickle.load(open(\"dm.pkl\", \"rb\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import copy\n",
                "# from cgflow.models.pocket import _PairwiseMessages, _InvariantEmbedding, SemlaLayer\n",
                "\n",
                "\n",
                "# class PocketEncoder(torch.nn.Module):\n",
                "\n",
                "#     def __init__(\n",
                "#         self,\n",
                "#         d_equi,\n",
                "#         d_inv,\n",
                "#         d_message,\n",
                "#         n_layers,\n",
                "#         n_attn_heads,\n",
                "#         d_message_ff,\n",
                "#         d_edge,\n",
                "#         n_atom_names,\n",
                "#         n_bond_types,\n",
                "#         n_res_types,\n",
                "#         n_charge_types=7,\n",
                "#         emb_size=64,\n",
                "#         fixed_equi=False,\n",
                "#         eps=1e-6,\n",
                "#     ):\n",
                "#         super().__init__()\n",
                "\n",
                "#         if fixed_equi and d_equi != 1:\n",
                "#             raise ValueError(\n",
                "#                 f\"If fixed_equi is True d_equi must be 1, got {d_equi}\")\n",
                "\n",
                "#         self.d_equi = d_equi\n",
                "#         self.d_inv = d_inv\n",
                "#         self.d_message = d_message\n",
                "#         self.n_layers = n_layers\n",
                "#         self.n_attn_heads = n_attn_heads\n",
                "#         self.d_message_ff = d_message_ff\n",
                "#         self.d_edge = d_edge\n",
                "#         self.emb_size = emb_size\n",
                "#         self.fixed_equi = fixed_equi\n",
                "#         self.eps = eps\n",
                "\n",
                "#         # Embedding and encoding modules\n",
                "#         self.inv_emb = _InvariantEmbedding(d_inv,\n",
                "#                                            n_atom_names,\n",
                "#                                            n_bond_types,\n",
                "#                                            emb_size,\n",
                "#                                            n_charge_types=n_charge_types,\n",
                "#                                            n_res_types=n_res_types)\n",
                "#         self.bond_emb = _PairwiseMessages(d_equi, d_inv, d_inv, d_message,\n",
                "#                                           d_edge, d_message_ff, emb_size)\n",
                "\n",
                "#         if fixed_equi is not None:\n",
                "#             self.coord_emb = torch.nn.Linear(1, d_equi, bias=False)\n",
                "\n",
                "#         # Create a stack of encoder layers\n",
                "#         layer = SemlaLayer(\n",
                "#             d_equi,\n",
                "#             d_inv,\n",
                "#             d_message,\n",
                "#             n_attn_heads,\n",
                "#             d_message_ff,\n",
                "#             d_self_edge_in=d_edge,\n",
                "#             fixed_equi=fixed_equi,\n",
                "#             zero_com=False,\n",
                "#             eps=eps,\n",
                "#         )\n",
                "\n",
                "#         layers = self._get_clones(layer, n_layers)\n",
                "#         self.layers = torch.nn.ModuleList(layers)\n",
                "\n",
                "#     @property\n",
                "#     def hparams(self):\n",
                "#         return {\n",
                "#             \"d_equi\": self.d_equi,\n",
                "#             \"d_inv\": self.d_inv,\n",
                "#             \"d_message\": self.d_message,\n",
                "#             \"n_layers\": self.n_layers,\n",
                "#             \"n_attn_heads\": self.n_attn_heads,\n",
                "#             \"d_message_ff\": self.d_message_ff,\n",
                "#             \"d_edge\": self.d_edge,\n",
                "#             \"emb_size\": self.emb_size,\n",
                "#             \"fixed_equi\": self.fixed_equi,\n",
                "#             \"eps\": self.eps,\n",
                "#         }\n",
                "\n",
                "#     def forward(self,\n",
                "#                 coords,\n",
                "#                 atom_names,\n",
                "#                 atom_charges,\n",
                "#                 res_types,\n",
                "#                 bond_types,\n",
                "#                 atom_mask=None):\n",
                "#         \"\"\"Encode the protein pocket into a learnable representation\n",
                "\n",
                "#         Args:\n",
                "#             coords (torch.Tensor): Coordinate tensor, shape [B, N, 3]\n",
                "#             atom_names (torch.Tensor): Atom name indices, shape [B, N]\n",
                "#             atom_charges (torch.Tensor): Atom charge indices, shape [B, N]\n",
                "#             residue_types (torch.Tensor): Residue type indices for each atom, shape [B, N]\n",
                "#             bond_types (torch.Tensor): Bond type indicies for each pair, shape [B, N, N]\n",
                "#             atom_mask (torch.Tensor): Mask for atoms, shape [B, N], 1 for real atom, 0 otherwise\n",
                "\n",
                "#         Returns:\n",
                "#             tuple[torch.Tensor, torch.Tensor]: Equivariant and invariant features, [B, N, 3, d_equi] and [B, N, d_inv]\n",
                "#         \"\"\"\n",
                "\n",
                "#         atom_mask = torch.ones_like(\n",
                "#             coords[..., 0]) if atom_mask is None else atom_mask\n",
                "#         # adj_matrix = smolF.adj_from_node_mask(atom_mask, self_connect=True)\n",
                "#         adj_matrix = smolF.edges_from_nodes(coords,\n",
                "#                                             k=None,\n",
                "#                                             node_mask=atom_mask,\n",
                "#                                             edge_format=\"adjacency\",\n",
                "#                                             self_connect=True)\n",
                "        \n",
                "#         coords = coords.unsqueeze(-1)\n",
                "#         equis = coords if self.fixed_equi else self.coord_emb(coords)\n",
                "\n",
                "#         invs, edges = self.inv_emb(atom_names,\n",
                "#                                    bond_types,\n",
                "#                                    atom_mask,\n",
                "#                                    atom_charges=atom_charges,\n",
                "#                                    res_types=res_types)\n",
                "#         edges = self.bond_emb(equis, invs, equis, invs, edges)\n",
                "#         edges = edges * adj_matrix.unsqueeze(-1)\n",
                "\n",
                "#         for layer in self.layers:\n",
                "#             equis, invs, _, _ = layer(equis, invs, edges, adj_matrix,\n",
                "#                                       atom_mask)\n",
                "\n",
                "#         return equis, invs\n",
                "\n",
                "#     def _get_clones(self, module, n):\n",
                "#         return [copy.deepcopy(module) for _ in range(n)]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from cgflow.models.pocket import LigandGenerator, PocketEncoder\n",
                "\n",
                "n_bond_types = 5\n",
                "# Initialize PocketEncoder\n",
                "pocket_enc = PocketEncoder(\n",
                "    d_equi=pocket_d_equi,\n",
                "    d_inv=pocket_d_inv,\n",
                "    d_message=d_message,\n",
                "    n_layers=pocket_n_layers,\n",
                "    n_attn_heads=n_attn_heads,\n",
                "    d_message_ff=d_message_hidden,\n",
                "    d_edge=d_edge,\n",
                "    n_atom_names=vocab.size,\n",
                "    n_bond_types=n_bond_types,\n",
                "    n_res_types=n_res_types,\n",
                "    fixed_equi=fixed_equi\n",
                ")\n",
                "\n",
                "# Initialize LigandGenerator\n",
                "egnn_gen = LigandGenerator(\n",
                "    d_equi=n_coord_sets,\n",
                "    d_inv=d_model,\n",
                "    d_message=d_message,\n",
                "    n_layers=n_layers,\n",
                "    n_attn_heads=n_attn_heads,\n",
                "    d_message_ff=d_message_hidden,\n",
                "    d_edge=d_edge,\n",
                "    n_atom_types=vocab.size,\n",
                "    n_bond_types=n_bond_types,\n",
                "    n_extra_atom_feats=n_extra_atom_feats,\n",
                "    self_cond=self_condition,\n",
                "    pocket_enc=pocket_enc\n",
                ").cuda()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "coords torch.Size([8, 28, 3])\n",
                        "atomics torch.Size([8, 28])\n",
                        "bonds torch.Size([8, 28, 28])\n",
                        "charges torch.Size([8, 28])\n",
                        "residues torch.Size([8, 28])\n",
                        "mask torch.Size([8, 28])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[08:08:13] Explicit valence for atom # 0 O, 6, is greater than permitted\n",
                        "[08:08:13] Explicit valence for atom # 0 O, 6, is greater than permitted\n",
                        "[08:08:13] Explicit valence for atom # 0 O, 6, is greater than permitted\n"
                    ]
                }
            ],
            "source": [
                "test_dl = dm.val_dataloader()\n",
                "for batch in test_dl:\n",
                "    prior, data, interpolated, masked_data, pockets, pocket_raw, t, rel_times, gen_times = batch\n",
                "    break\n",
                "\n",
                "for k, v in data.items():\n",
                "    print(k, v.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 105.38 GiB. GPU 0 has a total capacity of 44.53 GiB of which 36.40 GiB is free. Including non-PyTorch memory, this process has 8.12 GiB memory in use. Of the allocated memory 7.61 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m ligand_times \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, coords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m egnn_gen(\n\u001b[1;32m     19\u001b[0m     coords, atom_types, bond_types, atom_mask\u001b[38;5;241m=\u001b[39mmask, extra_feats\u001b[38;5;241m=\u001b[39mligand_times,\n\u001b[1;32m     20\u001b[0m     pocket_coords\u001b[38;5;241m=\u001b[39mpocket_coords, pocket_atom_names\u001b[38;5;241m=\u001b[39mpocket_atom_names,\n\u001b[1;32m     21\u001b[0m     pocket_atom_charges\u001b[38;5;241m=\u001b[39mpocket_atom_charges, pocket_res_types\u001b[38;5;241m=\u001b[39mpocket_res_types,\n\u001b[1;32m     22\u001b[0m     pocket_bond_types\u001b[38;5;241m=\u001b[39mpocket_bond_types, pocket_atom_mask\u001b[38;5;241m=\u001b[39mpocket_atom_mask\n\u001b[1;32m     23\u001b[0m )\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/projects/jlab/to.shen/cgflow-dev/src/cgflow/models/pocket.py:1630\u001b[0m, in \u001b[0;36mLigandGenerator.forward\u001b[0;34m(self, coords, atom_types, bond_types, atom_mask, extra_feats, cond_coords, cond_atomics, cond_bonds, pocket_coords, pocket_atom_names, pocket_atom_charges, pocket_res_types, pocket_bond_types, pocket_atom_mask, interactions)\u001b[0m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   1623\u001b[0m         pocket_coords, pocket_atom_names, pocket_atom_charges,\n\u001b[1;32m   1624\u001b[0m         pocket_res_types, pocket_bond_types\n\u001b[1;32m   1625\u001b[0m ]:\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll pocket inputs must be provided if the model is created with pocket cond.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1628\u001b[0m     )\n\u001b[0;32m-> 1630\u001b[0m pocket_equis, pocket_invs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m   1631\u001b[0m     pocket_coords,\n\u001b[1;32m   1632\u001b[0m     pocket_atom_names,\n\u001b[1;32m   1633\u001b[0m     pocket_atom_charges,\n\u001b[1;32m   1634\u001b[0m     pocket_res_types,\n\u001b[1;32m   1635\u001b[0m     pocket_bond_types,\n\u001b[1;32m   1636\u001b[0m     pocket_atom_mask\u001b[38;5;241m=\u001b[39mpocket_atom_mask,\n\u001b[1;32m   1637\u001b[0m )\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduplicate_pocket_equi:\n\u001b[1;32m   1640\u001b[0m     pocket_equis \u001b[38;5;241m=\u001b[39m pocket_equis\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_equi)\n",
                        "File \u001b[0;32m/projects/jlab/to.shen/cgflow-dev/src/cgflow/models/pocket.py:1674\u001b[0m, in \u001b[0;36mLigandGenerator.encode\u001b[0;34m(self, pocket_coords, pocket_atom_names, pocket_atom_charges, pocket_res_types, pocket_bond_types, pocket_atom_mask)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpocket_enc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1671\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call encode on a model initialised without a pocket encoder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1672\u001b[0m     )\n\u001b[0;32m-> 1674\u001b[0m pocket_equis, pocket_invs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpocket_enc(\n\u001b[1;32m   1675\u001b[0m     pocket_coords,\n\u001b[1;32m   1676\u001b[0m     pocket_atom_names,\n\u001b[1;32m   1677\u001b[0m     pocket_atom_charges,\n\u001b[1;32m   1678\u001b[0m     pocket_res_types,\n\u001b[1;32m   1679\u001b[0m     pocket_bond_types,\n\u001b[1;32m   1680\u001b[0m     atom_mask\u001b[38;5;241m=\u001b[39mpocket_atom_mask,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pocket_equis, pocket_invs\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/projects/jlab/to.shen/cgflow-dev/src/cgflow/models/pocket.py:1083\u001b[0m, in \u001b[0;36mPocketEncoder.forward\u001b[0;34m(self, coords, atom_names, atom_charges, res_types, bond_types, atom_mask)\u001b[0m\n\u001b[1;32m   1076\u001b[0m equis \u001b[38;5;241m=\u001b[39m coords \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_equi \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_emb(coords)\n\u001b[1;32m   1078\u001b[0m invs, edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minv_emb(atom_names,\n\u001b[1;32m   1079\u001b[0m                            bond_types,\n\u001b[1;32m   1080\u001b[0m                            atom_mask,\n\u001b[1;32m   1081\u001b[0m                            atom_charges\u001b[38;5;241m=\u001b[39matom_charges,\n\u001b[1;32m   1082\u001b[0m                            res_types\u001b[38;5;241m=\u001b[39mres_types)\n\u001b[0;32m-> 1083\u001b[0m edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbond_emb(equis, invs, equis, invs, edges)\n\u001b[1;32m   1084\u001b[0m edges \u001b[38;5;241m=\u001b[39m edges \u001b[38;5;241m*\u001b[39m adj_matrix\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m~/.conda/envs/broad/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/projects/jlab/to.shen/cgflow-dev/src/cgflow/models/pocket.py:215\u001b[0m, in \u001b[0;36m_PairwiseMessages.forward\u001b[0;34m(self, q_equi, q_inv, k_equi, k_inv, adj_matrix, edge_feats)\u001b[0m\n\u001b[1;32m    212\u001b[0m target_indices \u001b[38;5;241m=\u001b[39m edge_indices[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Project invariant features\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m q_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_message_proj(q_inv)[batch_indices, source_indices]\n\u001b[1;32m    216\u001b[0m k_messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_message_proj(k_inv)[batch_indices, target_indices]\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Compute dot products for selected edges only\u001b[39;00m\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 105.38 GiB. GPU 0 has a total capacity of 44.53 GiB of which 36.40 GiB is free. Including non-PyTorch memory, this process has 8.12 GiB memory in use. Of the allocated memory 7.61 GiB is allocated by PyTorch, and 18.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# Move data to device\n",
                "def to_device(*tensors):\n",
                "    return [t.to(device) for t in tensors]\n",
                "\n",
                "coords, atom_types, bond_types, mask = to_device(\n",
                "    data['coords'], data['atomics'], data['bonds'], data['mask']\n",
                ")\n",
                "pocket_coords, pocket_atom_names, pocket_atom_charges, pocket_res_types, pocket_bond_types, pocket_atom_mask = to_device(\n",
                "    pockets['coords'], pockets['atomics'], pockets['charges'],\n",
                "    pockets['residues'], pockets['bonds'], pockets['mask']\n",
                ")\n",
                "ligand_times = t.view(-1, 1, 1).expand(-1, coords.shape[1], -1).to(device)\n",
                "\n",
                "# Run model\n",
                "output = egnn_gen(\n",
                "    coords, atom_types, bond_types, atom_mask=mask, extra_feats=ligand_times,\n",
                "    pocket_coords=pocket_coords, pocket_atom_names=pocket_atom_names,\n",
                "    pocket_atom_charges=pocket_atom_charges, pocket_res_types=pocket_res_types,\n",
                "    pocket_bond_types=pocket_bond_types, pocket_atom_mask=pocket_atom_mask\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cgflow",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
